---
title: "Early Detection of Coronary Heart Disease Risk Using Supervised Machine Learning: 
         A Predictive Model Selection Approach"
author: "Yared Abraha"
date: "2025-02-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Final Project Machine Learning Model Development Workflow
```{r}
# Final Project Machine Learning Model Development Workflow
# Data set used: Framingham Heart Study Data set 
# --Load the Framingham heart study data set into R
# Step 1: Exploratory Data Analysis (EDA)
# -- EDA help understand the data structure, issues in the data, and distributions 
# 1.1 Explore the data set and data types 
# --Perform data/features metadata operation to make the data suitable for further analysis
# 1.2 Explore Issues in the Data 
# -Check for duplicates 
# -Check for missing values 
# -Check for distribution of the target variable
# -Check for outliers(extreme values) 
# 1.3 Data Analysis 
# -Descriptive Statistics (summary statistics of features)
# -Univariate Analysis (distribution of features) 
# -Bivariate Analysis (relationship between features and target)
# -Multivariate Analysis (relationship between multiple features)
# Step 2: Splitting the data set and Data Preparation for Modeling 
# 2.1 Split the data set (training/validation/testing)
# 2.2 Data Cleaning (Handling missing values)
# 2.3 Data Pre-processing 
# -Handling outliers
# -Handling target imbalance
# Step 3: Model Development 
# -Train model using training Set
# -Evaluate model using validation Set
# -Hyperparameter tuning (Cross Validation)
# -Retrain model with best hyperparameters found (class weights)
# Step 4: Model Evaluation 
# -Evaluate model using testing set 
# -Compare models and select the best model using appropriate metrics

```

# Load Libraries and Framingham data set
```{r}
# Libraries for clean Exploratory Data Analysis (EDA)
# Using tidyverse and ggplot2 for clean data analysis
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tibble)
# Load the Framingham data set into R using tidyverse readr 
framingham_original <- readr::read_csv("data/framingham_data.csv", show_col_types = FALSE)  

# keep the copy of the original data set to use it later (if needed) 
framing_data1 <- framingham_original

```

# Step1: Exploratory Data Analysis (EDA)
# 1.1 Explore the data set and data types
```{r}
## Step1: Exploratory Data Analysis (EDA)
# --Understand the Data structure of the Framingham heart study data set 
# 1.1 Explore the data set and data types
# Display the first fifteen rows of the data set 
head(framingham_original, n = 15)
# Display the full data set on a new window
View(framingham_original)

```

```{r}
# Explore number of rows and columns of the data set
cat("Total Rows:", nrow(framingham_original), "|Features(Predictors):", ncol(framingham_original) -1, 
    "|Target Variable(Predicted):", names(framingham_original)[ncol(framingham_original)], "\n")
# Column names of the data set 
names(framingham_original)
```
```{r}
# Summary statistics to explore correctness and spot issues
summary(framingham_original)
```
# Explore the data types
```{r}
# Check the data types of features (predictors) and target (predicted) 
glimpse(framingham_original)

```
```{r}
# Data/features metadata operation to make the data suitable for further analysis
# Change column names into appropriate, short, and easy to display names
# Use the copied data to keep the original safe (untouched)
framing_data1 <- framing_data1 %>% # using pip operator to chain operations and automate change of column name 
  rename(
    gender = male, # male is inappropriate (change it to gender)
    stroke = prevalentStroke, # stroke is shorter and explanatory
    hypertension = prevalentHyp, # hypertension is short and explanatory
    CHDRisk = TenYearCHD # the aim of the project is to detect CHD risk earlier
  )
# Display the updated column names 
names(framing_data1)  

```

```{r}
# Identify which features are numeric and which are categorical  
# Using Pip operator to chain operations and determine number of unique values of the features
framing_data1 %>%
  summarise(across(everything(), ~ n_distinct(.))) %>% # count number of unique values across rows
  pivot_longer(everything(), names_to = "Features", # display column name 1 and list of features
               values_to = "Number_Of_Unique_Values") %>%  # display column name 2 and number of unique values
  arrange(Number_Of_Unique_Values)

```

```{r}
# Determine between numeric and categorical features 
framing_data1 %>% # pip operator to chain functions 
  summarise(across(everything(), ~ n_distinct(.))) %>% # count unique values
  pivot_longer(everything(), names_to = "Features", # dispaly all features
               values_to = "Number_Of_Unique_Values") %>%  # count of unique values
         mutate(
           Variable_Type = ifelse(Number_Of_Unique_Values <= 5, "Categorical", # from previous analysis we know 5 is max 
           "Numeric")
           )%>%
  arrange(Variable_Type, Number_Of_Unique_Values)

```

# Change categorical features into Factor
```{r}
# Change categorical features into Factor for accurate analysis and interpretation 
# Prepare features for proper plotting and modeling
framing_factored <- framing_data1 %>% # keep the updated data set on new variable framing_factored
  mutate(across(where(~ n_distinct(.) <= 5), as.factor)) # change variables with 5 or less unique values into factor
str(framing_factored)

```

# 1.3 Explore issues in the data
```{r}
# 1.3 Explore issues in the data
# Check for duplicates 
any(duplicated(framing_factored))

```
# Check for Missing Values
```{r}
# Check for number and proportion of missing value in each column(feature)
# Using tibble object enframe for a clean output 
missing_per_feature <- enframe(colSums(is.na(framing_factored)),
                           name = "Feature",
                           value = "Number_of_Missing_Values")  %>% 
  mutate(Percentage_Per_Column = paste0(round((Number_of_Missing_Values / nrow(framing_factored)) * 100, 1), "%")) 
 print(missing_per_feature)

# Calculate percentage(proportion) of missing value per row
missing_row_proportion <- paste0(round(rowMeans(is.na(framing_factored))  * 100, 1), "%")
# Create a summary table of how many times each row proportion appears 
missing_per_row <- table(missing_row_proportion) %>% 
  as.data.frame() %>% # display it as a data frame
  rename(Missing_Row_Percentage = missing_row_proportion, Total_Number = Freq) %>% # frequency count gives total number  
  mutate(Percentage_of_Total_Rows = paste0(round((Total_Number / nrow(framing_factored)) * 100, 1), "%"))

print(missing_per_row)
# LOATS OF MISSING VALUES ARE FOUND!(Needs to be handled later on!)
```

# Check for distribution of the Target Variable(CHDRisk)
```{r}
# Check the distribution of the Target Variable
# To measure if there exist a class imbalance 
framing_factored %>% 
  count(CHDRisk) %>%
  mutate(Percentage = paste0(round(n/sum(n) * 100), "%")) 

```


```{r}
# Barplot showing the distribution of target variable CHDRisk
framing_factored %>% 
  count(CHDRisk) %>% # count number of 0s and 1s
  mutate(Percent = paste0(round(n/sum(n) * 100, 1)), # mutate calculate percentage of distribution
         CHDRisk = factor(CHDRisk, levels = c(0,1), labels = c("NoRisk", "HighRisk"))) %>% # change 0,1 labels to words
  ggplot(aes(x = CHDRisk, y = n, fill = CHDRisk)) + # ggplot assign x and y values and a legend (CHDRisk)
  geom_col(width = 0.5) + 
  geom_text(aes(label = paste0(n, " (", Percent, "%)")), vjust = -0.2, size = 3.5) + # display percentage on bars
  scale_fill_manual(values = c("skyblue", "brown"))+ # adjust color of the bars
  labs(title = "Distribution of Target Variable (CHDRisk)", x = "CHDRisk", y = "Count") +
  theme_minimal() # clean background for clear display

# THERE IS AHUGE CLASS IMBALANCE FOUND 0(NoRisk = 84.8%) and 1(HighRisk = 15.2%)
```
# Check for outliers(extreme values)
```{r}
# Check for outliers in the numeric features using a box plot
framing_factored %>% # explore through the whole data set
  select(where(is.numeric)) %>% # select only numeric features
  pivot_longer(everything()) %>% # include all data points and keep them in a long format
  ggplot(aes(x = "", y = value)) + # plot 
  geom_boxplot(fill = "skyblue") + # the box plot color is set to be sKyblue
  facet_wrap(~name, scales = "free") + # produce a grid display where all features and their outliers are displayed 
  theme_minimal() # clear background
# THERE ARE A LOT OF OUTLIERS IN ALL NUMERIC FEATURES EXCEPT AGE!
# NEEDS TO BE HANDLED LATER ON!

```
```{r}
# IQR method to compute outlier thresholds (computed for each features, glucose is considered here)
first_quartile <- quantile(framing_factored$glucose, 0.25, na.rm = TRUE) # the median of the lower half of the data
third_quartile <- quantile(framing_factored$glucose, 0.75, na.rm = TRUE) # the median of the upper half of the data
interquartile_range <- third_quartile - first_quartile # measure the mid spread of the data
# calculate lower and upper thresholds
lower_threshold <- first_quartile - 1.5 * interquartile_range # any value smaller than this is an outlier
upper_threshold <- third_quartile + 1.5 * interquartile_range # any value greater than this is an outlier

# Draw box plot to visually see the outliers 
# Detecting outliers in glucose
ggplot(framing_factored, aes(x = "", y = glucose)) + # extract glucose and count of glucose represent the y axis
  geom_boxplot(fill = "skyblue") + # the box plot is filled skyblue color
  geom_hline(yintercept = lower_threshold, linetype = "dashed", color = "red") + # draw the lower threshold 
  geom_hline(yintercept = upper_threshold, linetype = "dashed", color = "red") + # draw the upper threshold 
  labs(title = "Boxpot with red dashed outlier threshold to detect outliers in glucose") +
  theme_minimal()

```

```{r}
# Calculate number and percentage of outliers in the numeric features 
framing_factored %>%
  select(where(is.numeric)) %>% # select only numeric features
  # select all columns and summarise to rows
  summarise(across(everything(), ~{    # (~/tidle in R) operator introduce function to be applied on each feature
    first_q <- quantile(.x, 0.25, na.rm = TRUE) # na.rm = TRUE ignores the NA (misisng) values 
    third_q <- quantile(.x, 0.75, na.rm = TRUE)
    interquartile_r <- third_q - first_q 
    lower_thr <- first_q - 1.5 * interquartile_r
    upper_thr <- third_q + 1.5 * interquartile_r
    # for each feature (.x) count values that are less than the lower bound and or greater than the upper bound
    sum(.x < lower_thr | .x > upper_thr, na.rm = TRUE) # .x represent current value (feature) explored
  })) %>%
  pivot_longer(everything(), names_to = "Features", # converts to longer format and output column name
               values_to = "Number_Of_Outliers") %>% # output column name
  mutate(Percentage = round(Number_Of_Outliers / nrow(framing_factored), 3)) # percentage of outliers per feature

```






```{r}
# End of page
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
